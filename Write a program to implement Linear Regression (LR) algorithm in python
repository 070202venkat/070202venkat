import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error

Generate synthetic data

np.random.seed(42) X = 2 * np.random.rand(100, 1) y = 4 + 3 * X + np.random.randn(100, 1)

Split data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

Add bias term (column of ones) for Normal Equation method

X_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]

Normal Equation

theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)

Predictions using Normal Equation

X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test] y_pred_normal = X_test_b.dot(theta_best)

Gradient Descent Implementation

def gradient_descent(X, y, learning_rate=0.1, n_iterations=1000): m = len(y) theta = np.random.randn(2, 1)  # Initialize parameters randomly for iteration in range(n_iterations): gradients = (2/m) * X.T.dot(X.dot(theta) - y) theta -= learning_rate * gradients return theta

Add bias term for Gradient Descent

X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train] theta_gd = gradient_descent(X_train_b, y_train)

y_pred_gd = X_test_b.dot

